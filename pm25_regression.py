# -*- coding: utf-8 -*-
"""PM25_Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13AHR8mh0cskn-MyLRYzTNwLpO33NlDCK
"""

from keras.layers import Input
from keras.layers.core import Dense
from keras.models import Sequential
from keras.models import Model
from keras import regularizers
from keras import backend as K

from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

!wget https://github.com/iceman67/HRS/blob/master/April_sensor_data.csv

# read in that strange European CSV data, semi-colon separated, with commas for decimal points
aqdf = pd.read_csv("April_sensor_data.csv")

aqdf.columns

aqdf = aqdf.rename(columns={'Huminity': 'Humidity', 'PM2.5' :'PM25', 'PM10.0':'PM100'})

aqdf.drop('NO', axis=1, inplace=True)
aqdf.drop('REG_DATE', axis=1, inplace=True)
#aqdf.drop('PM2.5', axis=1, inplace=True)
#aqdf.drop('PM10.0', axis=1, inplace=True)
aqdf.drop('Temperature', axis=1, inplace=True)
aqdf.drop('TVOC', axis=1, inplace=True)
aqdf.drop('Humidity', axis=1, inplace=True)
aqdf.drop('CO2', axis=1, inplace=True)

"""The standard score of a sample x is calculated as:

z = (x - u) / s

Standardization of a dataset is a common requirement for many machine learning estimators.  

StandardScaler(x), 평균이 0과 표준편차가 1이 되도록 변환

https://datascienceschool.net/view-notebook/f43be7d6515b48c0beb909826993c856/
"""

#scaler =MinMaxScaler()
scaler = StandardScaler()
x_scaled = scaler.fit_transform(aqdf)

x_means = scaler.mean_
x_stds = scaler.scale_

x_means

x_stds

x_scaled[0]

x_scaled.shape

y = x_scaled[:, 0]                  # y is  CO2 column - the target

y

x = np.delete(x_scaled, 0, axis=1)  # x is everything else - the input

x.shape

x_stds[0]

x_means[0]

train_size = int(0.7 * x.shape[0])

x_train, x_test, y_train, y_test = x[0:train_size], x[train_size:], y[0:train_size], y[train_size:]

# regression network with 1 features -> 8 latent space -> 1 output

readings = Input(shape=(1, ))
encoded = Dense(8, activation='relu', kernel_initializer='glorot_uniform')(readings)
decoded = Dense(1, kernel_initializer='glorot_uniform')(encoded)
model = Model(inputs=[readings], outputs=[decoded])

model.compile(loss='mse', optimizer='adam')

my_epochs = 50

my_batch_size = 10
history = model.fit(x_train, y_train, batch_size=my_batch_size, epochs=my_epochs, validation_split=0.2)
y_test_pred = model.predict(x_test).flatten()

# Look at the results
restore_PM25 = lambda m: (m * x_stds[0]) + x_means[0]

for i in range(10):
    label = restore_PM25(y_test[i])
    prediction = restore_PM25(y_test_pred[i])
    print("PM 2.5 expected: {:.3f}, predicted: {:.3f}".format(label, prediction))

plt.figure(figsize=(12, 6), dpi=200)
plt.title("Entire PM 2.5 test set values against predictions")
plt.plot(np.arange(y_test.shape[0]), restore_PM25(y_test), color='b', label='PM2.5 actual')
plt.plot(np.arange(x_test.shape[0]), restore_PM25(x_test), color='g', label='PM10.0 actual')
#plt.plot(np.arange(x_test.shape[0]), x_test, color='g', label='Humity actual')
plt.plot(np.arange(y_test_pred.shape[0]), restore_PM25(y_test_pred), color='r', alpha=0.5, label='predicted')
plt.xlabel("time")
plt.ylabel("PM 2.5 concentrations (microg/m^3)")
plt.legend(loc='best')
plt.show()


K.clear_session()

plt.figure(figsize=(12, 6), dpi=100)
plt.title("Entire PM2.5 test set values against predictions")
plt.plot(np.arange(x_test.shape[0]), restore_benzine(x_test), color='b', label='actual')
plt.xlabel("time")
plt.ylabel("PM2.5 concentrations (microg/m^3)")
plt.legend(loc='best')
plt.show()